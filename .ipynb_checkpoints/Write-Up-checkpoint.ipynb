{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle Detection Project\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Perform a Histogram of Oriented Gradients (HOG) feature extraction on a labeled training set of images, also apply a color transform and append binned color features, as well as histograms of color, to the HOG feature vector and train a classifier Linear SVM classifier\n",
    "* Implement a sliding-window technique and use the trained classifier to search for vehicles in images.\n",
    "* Run the pipeline on a video stream and create a heat map of recurring detections frame by frame to reject outliers and follow detected vehicles.\n",
    "* Estimate a bounding box for vehicles detected.\n",
    "\n",
    "[//]: # (Image References)\n",
    "[image1]: ./output_images/car_noncar.png\n",
    "[image2]: ./output_images/Car_HOG_Features.png\n",
    "[image3]: ./output_images/Non_Car_HOG_Features.png\n",
    "[image4]: ./output_images/Feature_Normalization.png\n",
    "[image5]: ./output_images/window_grid_detection.png\n",
    "[image6]: ./output_images/heatmap.png\n",
    "[video1]: ./output_videos/project_video_annotated.mp4\n",
    "\n",
    "\n",
    "### Histogram of Oriented Gradients (HOG)\n",
    "\n",
    "#### 1. HOG Feature Extraction\n",
    "\n",
    "The code for this step is contained in the third code cell of the jupytor notebook. \n",
    "I started by reading in all the `vehicle` and `non-vehicle` images.  Here is an example of one of each of the `vehicle` and `non-vehicle` classes:\n",
    "\n",
    "![alt text][image1]\n",
    "\n",
    "I then explored different color spaces and different `skimage.hog()` parameters (`orientations`, `pixels_per_cell`, and `cells_per_block`).  I grabbed random images from each of the two classes and displayed them to get a feel for what the `skimage.hog()` output looks like.\n",
    "\n",
    "Here is an example using the `YCrCb` color space and HOG parameters of `orientations=9`, `pixels_per_cell=(8, 8)` and `cells_per_block=(2, 2)` on car and non-car images:\n",
    "\n",
    "\n",
    "![alt text][image2]\n",
    "![alt text][image3]\n",
    "\n",
    "#### 2. HOG parameters Selection\n",
    "\n",
    "The HOG parameters were narrowed down, at first by visual inspection of HOG images, looking at the prominence of the HOG features and later during the SVM network explained below.\n",
    "\n",
    "### Classifier Training\n",
    "\n",
    "I trained a linear SVM using an array of concatenated features extracted as HOG Features, spatial color information and color histogram on 1500 images per class from a set of about 8000 training images each of cars and non-cars from KITTI and GTI image database. The code for this part is in cell 5 of the jypyter notebook. During this training, I observed how changing the image colorscale from `RGB` to `YCrCb` improved the training accuracy. With training accuracies varying in the range of 98 - 99.5 %, I did not tweak the network training parameters any further. Before training the network, the feature vectors were normalized so that one particular set of feaures do not dominate the rest. An exampe of feature normalization is shown below:\n",
    "\n",
    "![alt text][image4]\n",
    "\n",
    "### Sliding Window Search\n",
    "\n",
    "I started with single scaling approach, using the Udacity class code as the basis for the algorithm. I did not realise the need to make modifications/use multiple scaling untill the video implementation explained below. Furthermore, as recommended in the class, I used the sliding window routine in a way that I don't have to extract hog features for each and every window, but do it all at once instead, and then simply use a subset of the extracted HOG features for the window under inspection. Furthermore, I only looked for cars in the lower half of image, which contains the road, thus saving time on processing. The code is in cell 6.\n",
    "\n",
    "Also, using all three techniques, namely YCrCb 3-channel HOG features plus spatially binned color and histograms of color in the feature vector provided a nice result. Below is a sample image which shows the window grid (8 x 8 cells) drawn on an unscaled image and how the detected vehicles are surrounded by blue square windows. Interestingly, even a car on the other side of the road is detected by the classifier as show below:\n",
    "![alt text][image5]\n",
    "\n",
    "---\n",
    "\n",
    "### Video Implementation\n",
    "\n",
    "Here's a [link to my video result](./output_videos/project_video_annotated.mp4)\n",
    "\n",
    "During the video implementation, I observed that the algorithm wouldn't detect cars once they move far away from the camera. Basically, as they get smaller in size, they get undetectable. Then instead of 1, I introduced 2 scales to scan the images. For the upper half of the image portion under consideration (lower half of image), I used samller scaling value, which scaled down the image less as compared to the lower portion, which is closer to the camera, where I used larger scaling value, as the cars closer to the camera, need to be scaled down relatively more than those far off.\n",
    "As we can see in the image above, there are multiple duplicate frames detected for the same car, so in order to get rid of duplicate frames, as well as any possibility of erratic false positive, I created a heatmap and then thresholded that map to identify vehicle positions.  I then used `scipy.ndimage.measurements.label()` to identify individual blobs in the heatmap.  I then assumed each blob corresponded to a vehicle.  I constructed bounding boxes to cover the area of each blob detected. The image below shows the same. Only the relatively 'hotter' portions of the image are considered for boxing the car.\n",
    "![alt text][image6]\n",
    "\n",
    "---\n",
    "\n",
    "### Discussion\n",
    "\n",
    "In a few frames, I am missing the detection. Furthermore, the detection windows are very erratic from frame to frame. In addition to this, there may be another failure mode possible which is false positives, detecting something as a car which is not. The first thing I would like to improve in this would be frame to frame tracking, somehow using the history as a measure to better track a vehicle instead of doing a blind search from scratch in an image. Secondly, if I have a better detection and tracking algorithm, I can then afford to have higher threshsolds in the heatmap to avoid false positves, which if currently increased, starts ignoring the road vehicles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:carnd-term1]",
   "language": "python",
   "name": "conda-env-carnd-term1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
